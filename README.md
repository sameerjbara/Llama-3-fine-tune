This project focuses on fine-tuning the Meta Llama 3.1-8B model to enhance its performance in summarizing dialogues, specifically using the SAMSum dataset. 
The project leveraged advanced techniques, including Low-Rank Adaptation (LoRA), to optimize the training process, making it more efficient and less resource-intensive.
The goal was to develop a model capable of generating accurate, concise, and contextually appropriate summaries from dialogues, which is critical in applications such as customer support, chatbots, and conversational AI.

The project involved multiple stages, including model configuration, data preprocessing, training, evaluation, and comparison with the base model. The final model was evaluated using BLEU scores on the test dataset to quantify its summarization capabilities.
